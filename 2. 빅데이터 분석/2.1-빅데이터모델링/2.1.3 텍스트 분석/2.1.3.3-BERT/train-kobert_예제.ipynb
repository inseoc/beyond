{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train-kobert-for-wellness.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NCpnYj2bfXvG"},"source":["# Wellness 심리 상담 데이터에 대한 KoBERT 학습 Question & Answer \n","데이터와 클래스 셋으로 이루어져, 질의 데이터가 들어왔을 때, Answer 클래스를 예측하도록 학습"]},{"cell_type":"markdown","metadata":{"id":"EN1oD-YEfukE"},"source":["## 1.Google Drive 연동\n","- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n","- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"]},{"cell_type":"markdown","metadata":{"id":"7TlLDiAJf0Zz"},"source":["### 1.1 GPU 연동 확인"]},{"cell_type":"code","metadata":{"id":"Q1qPeOGVfkb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620110164533,"user_tz":-540,"elapsed":650,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"7505d69f-b3f2-4c83-9ee8-41e52be6403c"},"source":["!nvidia-smi"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Tue May  4 06:36:04 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9tW2_JPaf79o"},"source":["### 1.2 Google Drive 연동\n","아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"]},{"cell_type":"code","metadata":{"id":"7voF7JHtf4J5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620110168498,"user_tz":-540,"elapsed":642,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"9a978a59-36f2-4505-cbfd-2cf393778c01"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T4MqQlXugAdI"},"source":["**Colab 디렉토리 아래 dialogLM 경로 확인**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mdqgpGkLgGoc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620110175925,"user_tz":-540,"elapsed":577,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"d6c12a90-be24-4bd2-810d-bcb57bd1968a"},"source":["!ls drive/MyDrive/bert_예제/"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WellnessConversation-LanguageModel-master\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wA36dTxFgMA8"},"source":["**필요 패키지 설치**"]},{"cell_type":"code","metadata":{"id":"OdbnMf8NjSHN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620110182159,"user_tz":-540,"elapsed":3946,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"a58ec198-0029-4ee8-b1e3-076a889a01bc"},"source":["!pip install -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: kobert-transformers==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 1)) (0.4.1)\n","Requirement already satisfied: kogpt2-transformers==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 2)) (0.3.0)\n","Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (3.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 4)) (1.8.1+cu101)\n","Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 5)) (0.8.1rc1)\n","Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 6)) (2.5.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (1.1.2)\n","Requirement already satisfied: flask_restful in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 8)) (0.3.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (0.0.45)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (20.9)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (0.1.95)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 4)) (3.7.4.3)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (7.1.2)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (2.11.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (1.1.0)\n","Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 8)) (9.0.1)\n","Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 8)) (1.15.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask_restful->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 8)) (2018.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->-r drive/MyDrive/bert_예제/WellnessConversation-LanguageModel-master/requirements.txt (line 7)) (1.1.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i51bCZyCjZzy"},"source":["## KoBERT QA Training"]},{"cell_type":"markdown","metadata":{"id":"UFSAkW6MkZb7"},"source":["**Path 추가**"]},{"cell_type":"code","metadata":{"id":"BRNM3Al6kdCI","executionInfo":{"status":"ok","timestamp":1620110216839,"user_tz":-540,"elapsed":524,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/dialogLM')"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrFfFlV8jfcg"},"source":["### 2.1 import package"]},{"cell_type":"code","metadata":{"id":"TS7um4UEsGdi","executionInfo":{"status":"ok","timestamp":1620110219801,"user_tz":-540,"elapsed":533,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n","from tqdm import tqdm\n","\n","import torch\n","from dialogLM.dataloader.wellness import WellnessTextClassificationDataset\n","from dialogLM.model.kobert import KoBERTforSequenceClassfication"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuH_ldfXsNn-"},"source":["**Train 함수**"]},{"cell_type":"code","metadata":{"id":"ELfj8XjhsM0n","executionInfo":{"status":"ok","timestamp":1620110227611,"user_tz":-540,"elapsed":593,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n","    losses = []\n","    train_start_index = train_step+1 if train_step != 0 else 0\n","    total_train_step = len(train_loader)\n","    model.train()\n","\n","    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n","        pbar.update(train_step)\n","        for i, data in enumerate(train_loader, train_start_index):\n","\n","            # data.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(**data)\n","\n","            loss = outputs[0]\n","\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.update(1)\n","            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n","\n","            if i >= total_train_step or i % save_step == 0:\n","                torch.save({\n","                    'epoch': epoch,  # 현재 학습 epoch\n","                    'model_state_dict': model.state_dict(),  # 모델 저장\n","                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n","                    'loss': loss.item(),  # Loss 저장\n","                    'train_step': i,  # 현재 진행한 학습\n","                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n","                }, save_ckpt_path)\n","\n","    return np.mean(losses)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZuWyjJEjxsF"},"source":["### KoBERT Question & Answer Training for Wellness dataset"]},{"cell_type":"code","metadata":{"id":"YeKLmjP4sn9j","executionInfo":{"status":"ok","timestamp":1620110350195,"user_tz":-540,"elapsed":533,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["from dialogLM.dataloader.wellness import WellnessAutoRegressiveDataset"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9vwUZQ9j0DN","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1620111373326,"user_tz":-540,"elapsed":1601,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"9d40287d-26ae-4e18-eb35-49a66f8a3168"},"source":["root_path='/content/drive/MyDrive/Colab Notebooks/dialogLM'\n","data_path = f\"{root_path}/data/QA_DATA.txt\"\n","checkpoint_path =f\"{root_path}/checkpoint\"\n","save_ckpt_path = f\"{checkpoint_path}/kobert-wellnesee-text-classification.pth\"\n","\n","n_epoch = 3          # Num of Epoch\n","batch_size = 2      # 배치 사이즈\n","ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(ctx)\n","save_step = 10 # 학습 저장 주기\n","learning_rate = 5e-6  # Learning Rate\n","\n","# WellnessTextClassificationDataset 데이터 로더\n","dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = KoBERTforSequenceClassfication()\n","model.to(device)\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","      'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","pre_epoch, pre_loss, train_step = 0, 0, 0\n","if os.path.isfile(save_ckpt_path):\n","    checkpoint = torch.load(save_ckpt_path, map_location=device)\n","    pre_epoch = checkpoint['epoch']\n","    # pre_loss = checkpoint['loss']\n","    train_step =  checkpoint['train_step']\n","    total_train_step =  checkpoint['total_train_step']\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n","    # best_epoch += 1\n","\n","losses = []\n","offset = pre_epoch\n","for step in range(n_epoch):\n","    epoch = step + offset\n","    loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n","    losses.append(loss)\n","\n","# data\n","data = {\n","    \"loss\": losses\n","}\n","df = pd.DataFrame(data)\n","display(df)\n","\n","# graph\n","plt.figure(figsize=[12, 4])\n","plt.plot(losses, label=\"loss\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"],"execution_count":45,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-ba95988ba1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# WellnessTextClassificationDataset 데이터 로더\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWellnessTextClassificationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/dialogLM/dataloader/wellness.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, num_label, device, max_seq_len, tokenizer)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;31m# Label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       data = {\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"xUv98Iba0EcO"},"source":["# **BERT CHAT**"]},{"cell_type":"code","metadata":{"id":"5ywqakGuj-qm","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1620110784375,"user_tz":-540,"elapsed":605,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"3b05c862-1b1d-42a6-ca0a-0ac7419eb951"},"source":["from torch import nn\n","from pytorch_transformers.modeling_bert import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n","\n","\n","class BertModel(nn.Module):\n","    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n","\n","    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n","        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n","\n","    model = modeling.BertModel(config=config)\n","    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config: BertConfig):\n","        \"\"\"Constructor for BertModel.\n","\n","        Args:\n","            config: `BertConfig` instance.\n","        \"\"\"\n","        super(BertModel, self).__init__()\n","        self.embeddings = BERTEmbeddings(config)\n","        self.encoder = BERTEncoder(config)\n","        self.pooler = BERTPooler(config)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, from_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.float()\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        embedding_output = self.embeddings(input_ids, token_type_ids)\n","        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n","        sequence_output = all_encoder_layers[-1]\n","        pooled_output = self.pooler(sequence_output)\n","        return all_encoder_layers, pooled_output"],"execution_count":35,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-8c5b4edbbf80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForPreTraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_tf_weights_in_bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"4pl17UiP0Pdc"},"source":["# 4.14 BertForQuestionAnswering"]},{"cell_type":"code","metadata":{"id":"mMCOeVJH0Kt_"},"source":["class BertForQuestionAnswering(nn.Module):\n","    \"\"\"BERT model for Question Answering (span extraction).\n","    This module is composed of the BERT model with a linear layer on top of\n","    the sequence output that computes start_logits and end_logits\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n","\n","    config = BertConfig(vocab_size=32000, hidden_size=512,\n","        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n","\n","    model = BertForQuestionAnswering(config)\n","    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertForQuestionAnswering, self).__init__()\n","        self.bert = BertModel(config)\n","        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n","        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","\n","        def init_weights(module):\n","            if isinstance(module, (nn.Linear, nn.Embedding)):\n","                # Slightly different from the TF version which uses truncated_normal for initialization\n","                # cf https://github.com/pytorch/pytorch/pull/5617\n","                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n","            elif isinstance(module, BERTLayerNorm):\n","                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n","                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n","            if isinstance(module, nn.Linear):\n","                module.bias.data.zero_()\n","        self.apply(init_weights)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n","        all_encoder_layers, _ = self.bert(input_ids, token_type_ids, attention_mask)\n","        sequence_output = all_encoder_layers[-1]\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        if start_positions is not None and end_positions is not None:\n","            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n","            start_positions = start_positions.squeeze(-1)\n","            end_positions = end_positions.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions.clamp_(0, ignored_index)\n","            end_positions.clamp_(0, ignored_index)\n","\n","            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            total_loss = (start_loss + end_loss) / 2\n","            return total_loss, (start_logits, end_logits)\n","        else:\n","            return start_logits, end_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4MnoeZuD9Wd","executionInfo":{"status":"ok","timestamp":1620111430910,"user_tz":-540,"elapsed":550,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":[" file = open('/content/drive/MyDrive/Colab Notebooks/dialogLM/data/QA_DATA.txt', 'r', encoding='utf-8')\n"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"HM8W5KzgKmMs","executionInfo":{"status":"error","timestamp":1620111866124,"user_tz":-540,"elapsed":653,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}},"outputId":"fae3b3be-2a68-48b8-ef3b-905e5e01ca32"},"source":["while True:\n","      line = file.readline()\n","      if not line:\n","        break\n","      datas = line.split(\"  \")\n","      index_of_words = tokenizer.encode(datas[0])\n","      token_type_ids = [0] * len(index_of_words)\n","      attention_mask = [1] * len(index_of_words)\n","\n","      # Padding Length\n","      padding_length = max_seq_len - len(index_of_words)\n","\n","      # Zero Padding\n","      index_of_words += [0] * padding_length\n","      token_type_ids += [0] * padding_length\n","      attention_mask += [0] * padding_length\n","\n","      # Label\n","      label = int(datas[1][:-1])"],"execution_count":60,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-4dbc2d55ac1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mindex_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tokenizer' has no attribute 'encode'"]}]},{"cell_type":"code","metadata":{"id":"_-WzHozpK_GX","executionInfo":{"status":"ok","timestamp":1620111723985,"user_tz":-540,"elapsed":738,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["from transformers import BertTokenizer"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbaAihydKtIz","executionInfo":{"status":"ok","timestamp":1620111542176,"user_tz":-540,"elapsed":552,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["import tokenizer"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVaenO8kLgZz","executionInfo":{"status":"ok","timestamp":1620111752225,"user_tz":-540,"elapsed":557,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["from keras.preprocessing.text import Tokenizer"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Idamb7fK3Az","executionInfo":{"status":"ok","timestamp":1620111862405,"user_tz":-540,"elapsed":582,"user":{"displayName":"Sun-Ho KIM","photoUrl":"","userId":"02175564280531097476"}}},"source":["from kobert_transformers import get_tokenizer"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-4XpvJwMSff"},"source":[""],"execution_count":null,"outputs":[]}]}